{
    "displayName": "硅基流动",
    "baseUrl": "https://api.siliconflow.cn/v1",
    "apiKeyTemplate": "sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx",
    "models": [
        {
            "id": "deepseek-ai/DeepSeek-V3.2-Exp",
            "name": "DeepSeek-V3.2-Exp (硅基流动)",
            "tooltip": "DeepSeek-V3.2-Exp 是一个实验性模型版本，作为迈向下一代架构的中间步骤。它在 V3.1-Terminus 的基础上引入了 DeepSeek 稀疏注意力（DeepSeek Sparse Attention，DSA）机制，这是一种旨在探索和验证长上下文场景下训练和推理效率优化的稀疏注意力机制。该实验性版本代表了团队在更高效 Transformer 架构方面的持续研究，特别关注于提升处理长文本序列时的计算效率。DSA 首次实现了细粒度的稀疏注意力，在保持模型输出质量几乎不变的同时，显著提升了长上下文训练和推理的效率",
            "maxInputTokens": 160000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "Pro/deepseek-ai/DeepSeek-V3.2-Exp",
            "name": "DeepSeek-V3.2-Exp (Pro) (硅基流动)",
            "tooltip": "DeepSeek-V3.2-Exp 是一个实验性模型版本，作为迈向下一代架构的中间步骤。它在 V3.1-Terminus 的基础上引入了 DeepSeek 稀疏注意力（DeepSeek Sparse Attention，DSA）机制，这是一种旨在探索和验证长上下文场景下训练和推理效率优化的稀疏注意力机制。该实验性版本代表了团队在更高效 Transformer 架构方面的持续研究，特别关注于提升处理长文本序列时的计算效率。DSA 首次实现了细粒度的稀疏注意力，在保持模型输出质量几乎不变的同时，显著提升了长上下文训练和推理的效率",
            "maxInputTokens": 160000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "deepseek-ai/DeepSeek-V3.1-Terminus",
            "name": "DeepSeek-V3.1-Terminus (硅基流动)",
            "tooltip": "DeepSeek-V3.1-Terminus 是由深度求索（DeepSeek）发布的 V3.1 模型的更新版本，定位为混合智能体大语言模型。此次更新在保持模型原有能力的基础上，专注于修复用户反馈的问题并提升稳定性。它显著改善了语言一致性，减少了中英文混用和异常字符的出现。模型集成了\"思考模式\"（Thinking Mode）和\"非思考模式\"（Non-thinking Mode），用户可通过聊天模板灵活切换以适应不同任务。作为一个重要的优化，V3.1-Terminus 增强了代码智能体（Code Agent）和搜索智能体（Search Agent）的性能，使其在工具调用和执行多步复杂任务方面更加可靠。",
            "maxInputTokens": 160000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "Pro/deepseek-ai/DeepSeek-V3.1-Terminus",
            "name": "DeepSeek-V3.1-Terminus (Pro) (硅基流动)",
            "tooltip": "DeepSeek-V3.1-Terminus 是由深度求索（DeepSeek）发布的 V3.1 模型的更新版本，定位为混合智能体大语言模型。此次更新在保持模型原有能力的基础上，专注于修复用户反馈的问题并提升稳定性。它显著改善了语言一致性，减少了中英文混用和异常字符的出现。模型集成了\"思考模式\"（Thinking Mode）和\"非思考模式\"（Non-thinking Mode），用户可通过聊天模板灵活切换以适应不同任务。作为一个重要的优化，V3.1-Terminus 增强了代码智能体（Code Agent）和搜索智能体（Search Agent）的性能，使其在工具调用和执行多步复杂任务方面更加可靠。",
            "maxInputTokens": 160000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "deepseek-ai/DeepSeek-V3.1",
            "name": "DeepSeek-V3.1 (硅基流动)",
            "tooltip": "DeepSeek-V3.1 是由深度求索（DeepSeek AI）发布的混合模式大语言模型，它在前代模型的基础上进行了多方面的重要升级。该模型的一大创新是集成了\"思考模式\"（Thinking Mode）和\"非思考模式\"（Non-thinking Mode）于一体，用户可以通过调整聊天模板灵活切换，以适应不同的任务需求。通过专门的训练后优化，V3.1 在工具调用和 Agent 任务方面的性能得到了显著增强，能够更好地支持外部搜索工具和执行多步复杂任务。该模型基于 DeepSeek-V3.1-Base 进行后训练，通过两阶段长文本扩展方法，大幅增加了训练数据量，使其在处理长文档和长篇代码方面表现更佳。作为一个开源模型，DeepSeek-V3.1 在编码、数学和推理等多个基准测试中展现了与顶尖闭源模型相媲美的能力，同时凭借其混合专家（MoE）架构，在保持巨大模型容量的同时，有效降低了推理成本。",
            "maxInputTokens": 160000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "Pro/deepseek-ai/DeepSeek-V3.1",
            "name": "DeepSeek-V3.1 (Pro) (硅基流动)",
            "tooltip": "DeepSeek-V3.1 是由深度求索（DeepSeek AI）发布的混合模式大语言模型，它在前代模型的基础上进行了多方面的重要升级。该模型的一大创新是集成了\"思考模式\"（Thinking Mode）和\"非思考模式\"（Non-thinking Mode）于一体，用户可以通过调整聊天模板灵活切换，以适应不同的任务需求。通过专门的训练后优化，V3.1 在工具调用和 Agent 任务方面的性能得到了显著增强，能够更好地支持外部搜索工具和执行多步复杂任务。该模型基于 DeepSeek-V3.1-Base 进行后训练，通过两阶段长文本扩展方法，大幅增加了训练数据量，使其在处理长文档和长篇代码方面表现更佳。作为一个开源模型，DeepSeek-V3.1 在编码、数学和推理等多个基准测试中展现了与顶尖闭源模型相媲美的能力，同时凭借其混合专家（MoE）架构，在保持巨大模型容量的同时，有效降低了推理成本。",
            "maxInputTokens": 160000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "deepseek-ai/DeepSeek-V3",
            "name": "DeepSeek-V3 (硅基流动)",
            "tooltip": "新版 DeepSeek-V3 （DeepSeek-V3-0324）与之前的 DeepSeek-V3-1226 使用同样的 base 模型，仅改进了后训练方法。新版 V3 模型借鉴 DeepSeek-R1 模型训练过程中所使用的强化学习技术，大幅提高了在推理类任务上的表现水平，在数学、代码类相关评测集上取得了超过 GPT-4.5 的得分成绩。此外该模型在工具调用、角色扮演、问答闲聊等方面也得到了一定幅度的能力提升。",
            "maxInputTokens": 160000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "Pro/deepseek-ai/DeepSeek-V3",
            "name": "DeepSeek-V3 (Pro) (硅基流动)",
            "tooltip": "新版 DeepSeek-V3 （DeepSeek-V3-0324）与之前的 DeepSeek-V3-1226 使用同样的 base 模型，仅改进了后训练方法。新版 V3 模型借鉴 DeepSeek-R1 模型训练过程中所使用的强化学习技术，大幅提高了在推理类任务上的表现水平，在数学、代码类相关评测集上取得了超过 GPT-4.5 的得分成绩。此外该模型在工具调用、角色扮演、问答闲聊等方面也得到了一定幅度的能力提升。",
            "maxInputTokens": 160000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "zai-org/GLM-4.6",
            "name": "GLM-4.6 (硅基流动)",
            "tooltip": "与 GLM-4.5 相比，GLM-4.6 带来了多项关键改进。其上下文窗口从 128K 扩展到 200K tokens，使模型能够处理更复杂的智能体任务。模型在代码基准测试中取得了更高的分数，并在 Claude Code、Cline、Roo Code 和 Kilo Code 等应用中展现了更强的真实世界性能，包括在生成视觉效果精致的前端页面方面有所改进。GLM-4.6 在推理性能上表现出明显提升，并支持在推理过程中使用工具，从而带来了更强的综合能力。它在工具使用和基于搜索的智能体方面表现更强，并且能更有效地集成到智能体框架中。在写作方面，该模型在风格和可读性上更符合人类偏好，并在角色扮演场景中表现得更自然。",
            "maxInputTokens": 200000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "zai-org/GLM-4.5",
            "name": "GLM-4.5 (硅基流动)",
            "tooltip": "GLM-4.5 是一款专为智能体应用打造的基础模型，使用了混合专家（Mixture-of-Experts）架构。在工具调用、网页浏览、软件工程、前端编程领域进行了深度优化，支持无缝接入 Claude Code、Roo Code 等代码智能体中使用。GLM-4.5 采用混合推理模式，可以适应复杂推理和日常使用等多种应用场景。",
            "maxInputTokens": 128000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "zai-org/GLM-4.5-Air",
            "name": "GLM-4.5-Air (硅基流动)",
            "tooltip": "GLM-4.5-Air 是一款专为智能体应用打造的基础模型，使用了混合专家（Mixture-of-Experts）架构。在工具调用、网页浏览、软件工程、前端编程领域进行了深度优化，支持无缝接入 Claude Code、Roo Code 等代码智能体中使用。GLM-4.5-Air 采用混合推理模式，可以适应复杂推理和日常使用等多种应用场景。",
            "maxInputTokens": 128000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "moonshotai/Kimi-K2-Instruct-0905",
            "name": "Kimi-K2-Instruct-0905 (硅基流动)",
            "tooltip": "Kimi K2-Instruct-0905 是 Kimi K2 最新、最强大的版本。它是一款顶尖的混合专家（MoE）语言模型，拥有 1 万亿的总参数和 320 亿的激活参数。该模型的主要特性包括：增强的智能体编码智能，在公开基准测试和真实世界的编码智能体任务中表现出显著的性能提升；改进的前端编码体验，在前端编程的美观性和实用性方面均有进步。",
            "maxInputTokens": 256000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "Pro/moonshotai/Kimi-K2-Instruct-0905",
            "name": "Kimi-K2-Instruct-0905 (Pro) (硅基流动)",
            "tooltip": "Kimi K2-Instruct-0905 是 Kimi K2 最新、最强大的版本。它是一款顶尖的混合专家（MoE）语言模型，拥有 1 万亿的总参数和 320 亿的激活参数。该模型的主要特性包括：增强的智能体编码智能，在公开基准测试和真实世界的编码智能体任务中表现出显著的性能提升；改进的前端编码体验，在前端编程的美观性和实用性方面均有进步。",
            "maxInputTokens": 256000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "Qwen/Qwen3-VL-30B-A3B-Instruct",
            "name": "Qwen3-VL-30B-A3B-Instruct (硅基流动)",
            "tooltip": "Qwen3-VL 是 Qwen 系列迄今为止最强大的视觉语言模型。该模型进行了全面升级，包括卓越的文本理解与生成、更深层次的视觉感知与推理、更长的上下文长度、增强的空间和视频动态理解，以及更强的智能体交互能力。作为基于混合专家（MoE）架构的指令微调（Instruct）版本，它专为灵活、按需部署而设计，具备强大的视觉智能体、视觉编码和视频理解能力，并原生支持 256K 上下文",
            "maxInputTokens": 256000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": true
            }
        },
        {
            "id": "Qwen/Qwen3-VL-30B-A3B-Thinking",
            "name": "Qwen3-VL-30B-A3B-Thinking (硅基流动)",
            "tooltip": "Qwen3-VL 是 Qwen 系列迄今为止最强大的视觉语言模型。该模型进行了全面升级，包括卓越的文本理解与生成、更深层次的视觉感知与推理、更长的上下文长度、增强的空间和视频动态理解，以及更强的智能体交互能力。这个推理增强的 “Thinking” 版本基于混合专家（MoE）架构构建，擅长执行操作 PC/移动设备图形用户界面、从图像生成代码以及在 STEM 领域进行高级多模态推理等任务。它原生支持 256K 上下文长度，并拥有支持 32 种语言的扩展 OCR 能力",
            "maxInputTokens": 256000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": true
            }
        },
        {
            "id": "Qwen/Qwen3-VL-235B-A22B-Instruct",
            "name": "Qwen3-VL-235B-A22B-Instruct (硅基流动)",
            "tooltip": "Qwen3-VL-235B-A22B-Instruct 是一个拥有 2350 亿参数的混合专家（MoE）视觉语言模型，其中激活的参数量为 220 亿。它是 Qwen3-VL-235B-A22B 的指令微调版本，专为聊天应用进行了优化。Qwen3-VL 是一系列接受文本和图像输入的多模态模型，经过大量数据训练。它在理解和推理文本与图像方面表现出先进的能力",
            "maxInputTokens": 256000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": true
            }
        },
        {
            "id": "Qwen/Qwen3-VL-235B-A22B-Thinking",
            "name": "Qwen3-VL-235B-A22B-Thinking (硅基流动)",
            "tooltip": "Qwen3-VL 是通义千问系列迄今最强大的视觉语言模型。该系列在文本理解与生成、视觉感知与推理、上下文长度、空间关系和视频动态理解以及 AI Agent 交互方面均实现了全面升级。Qwen3-VL-235B-A22B-Thinking 是该系列的旗舰模型之一，为推理增强的“思考”版本，在数学、因果分析和逻辑推理等多个多模态推理基准测试中取得了业界顶尖（SOTA）的表现。该模型是一个混合专家（MoE）架构，总参数量为 235B，激活参数量为 22B。它原生支持 256K 的上下文长度，并可扩展至 100 万，能够处理整本教科书或数小时的视频内容。此外，该模型具备强大的视觉 Agent 能力，可以操作桌面及移动端图形界面（GUI）、将草图转换为代码，并支持 3D 接地，为复杂的空间推理和具身智能应用奠定了基础",
            "maxInputTokens": 256000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": true
            }
        },
        {
            "id": "Qwen/Qwen3-Next-80B-A3B-Instruct",
            "name": "Qwen3-Next-80B-A3B-Instruct (硅基流动)",
            "tooltip": "Qwen3-Next-80B-A3B-Instruct 是由阿里巴巴通义千问团队发布的下一代基础模型。它基于全新的 Qwen3-Next 架构，旨在实现极致的训练和推理效率。该模型采用了创新的混合注意力机制（Gated DeltaNet 和 Gated Attention）、高稀疏度混合专家（MoE）结构以及多项训练稳定性优化。作为一个拥有 800 亿总参数的稀疏模型，它在推理时仅需激活约 30 亿参数，从而大幅降低了计算成本，并在处理超过 32K tokens 的长上下文任务时，推理吞吐量比 Qwen3-32B 模型高出 10 倍以上。此模型为指令微调版本，专为通用任务设计，不支持思维链（Thinking）模式。在性能上，它与通义千问的旗舰模型 Qwen3-235B 在部分基准测试中表现相当，尤其在超长上下文任务中展现出明显优势。",
            "maxInputTokens": 256000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "Qwen/Qwen3-Next-80B-A3B-Thinking",
            "name": "Qwen3-Next-80B-A3B-Thinking (硅基流动)",
            "tooltip": "Qwen3-Next-80B-A3B-Thinking 是由阿里巴巴通义千问团队发布的、专为复杂推理任务设计的下一代基础模型。它基于创新的 Qwen3-Next 架构，该架构融合了混合注意力机制（Gated DeltaNet 与 Gated Attention）和高稀疏度混合专家（MoE）结构，旨在实现极致的训练与推理效率。作为一个总参数达 800 亿的稀疏模型，它在推理时仅激活约 30 亿参数，大幅降低了计算成本，在处理超过 32K tokens 的长上下文任务时，吞吐量比 Qwen3-32B 模型高出 10 倍以上。此\"Thinking\"版本专为执行数学证明、代码综合、逻辑分析和规划等高难度多步任务而优化，并默认以结构化的\"思维链\"形式输出推理过程。在性能上，它不仅超越了 Qwen3-32B-Thinking 等成本更高的模型，还在多个基准测试中优于 Gemini-2.5-Flash-Thinking。",
            "maxInputTokens": 256000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "Qwen/Qwen3-Coder-30B-A3B-Instruct",
            "name": "Qwen3-Coder-30B-A3B-Instruct (硅基流动)",
            "tooltip": "Qwen3-Coder-30B-A3B-Instruct 是由阿里巴巴通义千问团队开发的 Qwen3 系列中的代码模型。作为一个经过精简优化的模型，它在保持高性能和高效率的同时，专注于提升代码处理能力。该模型在代理式编程（Agentic Coding）、自动化浏览器操作和工具调用等复杂任务上，于开源模型中表现出显著的性能优势。它原生支持 256K tokens 的长上下文，并可扩展至 1M tokens，从而能够更好地进行代码库级别的理解和处理。此外，该模型为 Qwen Code、CLINE 等平台提供了强大的代理编码支持，并设计了专门的函数调用格式。",
            "maxInputTokens": 256000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "Qwen/Qwen3-Coder-480B-A35B-Instruct",
            "name": "Qwen3-Coder-480B-A35B (硅基流动)",
            "tooltip": "Qwen3-Coder-480B-A35B-Instruct 是由阿里巴巴发布的、迄今为止最具代理（Agentic）能力的代码模型。它是一个拥有 4800 亿总参数和 350 亿激活参数的混合专家（MoE）模型，在效率和性能之间取得了平衡。该模型原生支持 256K（约 26 万） tokens 的上下文长度，并可通过 YaRN 等外推方法扩展至 100 万 tokens，使其能够处理大规模代码库和复杂的编程任务。Qwen3-Coder 专为代理式编码工作流设计，不仅能生成代码，还能与开发工具和环境自主交互，以解决复杂的编程问题。在多个编码和代理任务的基准测试中，该模型在开源模型中取得了顶尖水平，其性能可与 Claude Sonnet 4 等领先模型相媲美。此外，阿里还开源了配套的命令行工具 Qwen Code，以充分释放其强大的代理编程能力。",
            "maxInputTokens": 256000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "Qwen/Qwen3-30B-A3B-Instruct-2507",
            "name": "Qwen3-30B-A3B-Instruct-2507 (硅基流动)",
            "tooltip": "Qwen3-30B-A3B-Instruct-2507 是 Qwen3-30B-A3B 非思考模式的更新版本。这是一个拥有 305 亿总参数和 33 亿激活参数的混合专家（MoE）模型。该模型在多个方面进行了关键增强，包括显著提升了指令遵循、逻辑推理、文本理解、数学、科学、编码和工具使用等通用能力。同时，它在多语言的长尾知识覆盖范围上取得了实质性进展，并能更好地与用户在主观和开放式任务中的偏好对齐，从而能够生成更有帮助的回复和更高质量的文本。此外，该模型的长文本理解能力也增强到了 256K。此模型仅支持非思考模式，其输出中不会生成 `<think></think>` 标签。",
            "maxInputTokens": 256000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "Qwen/Qwen3-30B-A3B-Thinking-2507",
            "name": "Qwen3-30B-A3B-Thinking-2507 (硅基流动)",
            "tooltip": "Qwen3-30B-A3B-Thinking-2507 是由阿里巴巴通义千问团队发布的 Qwen3 系列的最新思考模型。作为一个拥有 305 亿总参数和 33 亿激活参数的混合专家（MoE）模型，它专注于提升复杂任务的处理能力。该模型在逻辑推理、数学、科学、编程和需要人类专业知识的学术基准测试上表现出显著的性能提升。同时，它在指令遵循、工具使用、文本生成和与人类偏好对齐等通用能力方面也得到了显著增强。模型原生支持 256K 的长上下文理解能力，并可扩展至 100 万 tokens。此版本专为\"思考模式\"设计，旨在通过详尽的逐步推理来解决高度复杂的任务，其 Agent 智能体能力也表现出色。",
            "maxInputTokens": 256000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "Qwen/Qwen3-235B-A22B",
            "name": "Qwen3-235B-A22B (硅基流动)",
            "tooltip": "Qwen3-235B-A22B 是通义千问系列的最新大语言模型，采用混合专家（MoE）架构，拥有 235B 总参数量和 22B 激活参数量。该模型独特地支持在思考模式（适用于复杂逻辑推理、数学和编程）和非思考模式（适用于高效的通用对话）之间无缝切换，显著增强了推理能力。模型在数学、代码生成和常识逻辑推理上表现优异，并在创意写作、角色扮演和多轮对话等方面展现出卓越的人类偏好对齐能力。此外，该模型支持 100 多种语言和方言，具备出色的多语言指令遵循和翻译能力",
            "maxInputTokens": 128000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "Qwen/Qwen3-235B-A22B-Instruct-2507",
            "name": "Qwen3-235B-A22B-Instruct-2507 (硅基流动)",
            "tooltip": "Qwen3-235B-A22B-Instruct-2507 是由阿里云通义千问团队开发的 Qwen3 系列中的一款旗舰级混合专家（MoE）大语言模型。该模型拥有 2350 亿总参数，每次推理激活 220 亿参数。它是作为 Qwen3-235B-A22B 非思考模式的更新版本发布的，专注于在指令遵循、逻辑推理、文本理解、数学、科学、编程及工具使用等通用能力上实现显著提升。此外，模型增强了对多语言长尾知识的覆盖，并能更好地对齐用户在主观和开放性任务上的偏好，以生成更有帮助和更高质量的文本。值得注意的是，该模型原生支持 256K（即 262,144 tokens）的超长上下文窗口，强化了其处理复杂长文本的能力。此版本仅支持非思考模式，不再生成 <think> 模块，旨在为直接问答、知识检索等任务提供更高效和精准的响应。",
            "maxInputTokens": 256000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "Qwen/Qwen3-235B-A22B-Thinking-2507",
            "name": "Qwen3-235B-A22B-Thinking-2507 (硅基流动)",
            "tooltip": "Qwen3-235B-A22B-Thinking-2507 是由阿里巴巴通义千问团队开发的 Qwen3 系列大型语言模型中的一员，专注于高难度的复杂推理任务。该模型基于混合专家（MoE）架构，总参数量达 2350 亿，而在处理每个 token 时仅激活约 220 亿参数，从而在保持强大性能的同时提高了计算效率。作为一个专门的\"思考\"模型，它在逻辑推理、数学、科学、编程和学术基准测试等需要人类专业知识的任务上表现显著提升，达到了开源思考模型中的顶尖水平。此外，模型还增强了通用能力，如指令遵循、工具使用和文本生成，并原生支持 256K 的长上下文理解能力，非常适合用于需要深度推理和处理长文档的场景。",
            "maxInputTokens": 256000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "Qwen/Qwen3-30B-A3B",
            "name": "Qwen3-30B-A3B (硅基流动)",
            "tooltip": "Qwen3-30B-A3B 是通义千问系列的最新大语言模型，采用混合专家（MoE）架构，拥有 30.5B 总参数量和 3.3B 激活参数量。该模型独特地支持在思考模式（适用于复杂逻辑推理、数学和编程）和非思考模式（适用于高效的通用对话）之间无缝切换，显著增强了推理能力。模型在数学、代码生成和常识逻辑推理上表现优异，并在创意写作、角色扮演和多轮对话等方面展现出卓越的人类偏好对齐能力。此外，该模型支持 100 多种语言和方言，具备出色的多语言指令遵循和翻译能力",
            "maxInputTokens": 128000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "Qwen/Qwen3-32B",
            "name": "Qwen3-32B (硅基流动)",
            "tooltip": "Qwen3-32B 是通义千问系列的最新大语言模型，拥有 32.8B 参数量。该模型独特地支持在思考模式（适用于复杂逻辑推理、数学和编程）和非思考模式（适用于高效的通用对话）之间无缝切换，显著增强了推理能力。模型在数学、代码生成和常识逻辑推理上表现优异，并在创意写作、角色扮演和多轮对话等方面展现出卓越的人类偏好对齐能力。此外，该模型支持 100 多种语言和方言，具备出色的多语言指令遵循和翻译能力",
            "maxInputTokens": 128000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "Qwen/Qwen3-14B",
            "name": "Qwen3-14B (硅基流动)",
            "tooltip": "Qwen3-14B 是通义千问系列的最新大语言模型，拥有 14.8B 参数量。该模型独特地支持在思考模式（适用于复杂逻辑推理、数学和编程）和非思考模式（适用于高效的通用对话）之间无缝切换，显著增强了推理能力。模型在数学、代码生成和常识逻辑推理上表现优异，并在创意写作、角色扮演和多轮对话等方面展现出卓越的人类偏好对齐能力。此外，该模型支持 100 多种语言和方言，具备出色的多语言指令遵循和翻译能力",
            "maxInputTokens": 128000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "Qwen/Qwen3-8B",
            "name": "Qwen3-8B (硅基流动)",
            "tooltip": "Qwen3-8B 是通义千问系列的最新大语言模型，拥有 8.2B 参数量。该模型独特地支持在思考模式（适用于复杂逻辑推理、数学和编程）和非思考模式（适用于高效的通用对话）之间无缝切换，显著增强了推理能力。模型在数学、代码生成和常识逻辑推理上表现优异，并在创意写作、角色扮演和多轮对话等方面展现出卓越的人类偏好对齐能力。此外，该模型支持 100 多种语言和方言，具备出色的多语言指令遵循和翻译能力",
            "maxInputTokens": 128000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "Qwen/QwQ-32B",
            "name": "QwQ-32B (硅基流动)",
            "tooltip": "QwQ 是 Qwen 系列的推理模型。与传统的指令调优模型相比，QwQ 具备思考和推理能力，能够在下游任务中实现显著增强的性能，尤其是在解决困难问题方面。QwQ-32B 是中型推理模型，能够在与最先进的推理模型（如 DeepSeek-R1、o1-mini）的对比中取得有竞争力的性能。该模型采用 RoPE、SwiGLU、RMSNorm 和 Attention QKV bias 等技术，具有 64 层网络结构和 40 个 Q 注意力头（GQA 架构中 KV 为 8 个）",
            "maxInputTokens": 128000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "inclusionAI/Ring-flash-2.0",
            "name": "Ring-flash-2.0 (硅基流动)",
            "tooltip": "Ring-flash-2.0 是一个基于 Ling-flash-2.0-base 深度优化的高性能思考模型。它采用混合专家（MoE）架构，总参数量为 100B，但在每次推理中仅激活 6.1B 参数。该模型通过独创的 icepop 算法，解决了 MoE 大模型在强化学习（RL）训练中的不稳定性难题，使其复杂推理能力在长周期训练中得以持续提升。Ring-flash-2.0 在数学竞赛、代码生成和逻辑推理等多个高难度基准测试中取得了显著突破，其性能不仅超越了 40B 参数规模以下的顶尖稠密模型，还能媲美更大规模的开源 MoE 模型及闭源的高性能思考模型。尽管该模型专注于复杂推理，它在创意写作等任务上也表现出色。此外，得益于其高效的架构设计，Ring-flash-2.0 在提供强大性能的同时，也实现了高速推理，显著降低了思考模型在高并发场景下的部署成本。",
            "maxInputTokens": 128000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": false,
                "imageInput": false
            }
        },
        {
            "id": "inclusionAI/Ling-flash-2.0",
            "name": "Ling-flash-2.0 (硅基流动)",
            "tooltip": "Ling-flash-2.0 是由蚂蚁集团百灵团队发布的 Ling 2.0 架构系列的第三款模型。它是一款混合专家（MoE）模型，总参数规模达到 1000 亿，但每个 token 仅激活 61 亿参数（非词向量激活 48 亿）。作为一个轻量级配置的模型，Ling-flash-2.0 在多个权威评测中展现出媲美甚至超越 400 亿级别稠密（Dense）模型及更大规模 MoE 模型的性能。该模型旨在通过极致的架构设计与训练策略，在\"大模型等于大参数\"的共识下探索高效能的路径。",
            "maxInputTokens": 128000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "inclusionAI/Ling-mini-2.0",
            "name": "Ling-mini-2.0 (硅基流动)",
            "tooltip": "Ling-mini-2.0 是一款基于 MoE 架构 的小尺寸高性能大语言模型。它拥有 16B 总参数，但每个 token 仅激活 1.4B（non-embedding 789M），从而实现了极高的生成速度。得益于高效的 MoE 设计与大规模高质量训练数据，尽管激活参数仅为 1.4B，Ling-mini-2.0 依然在下游任务中展现出可媲美 10B 以下 dense LLM 及更大规模 MoE 模型的顶尖性能。",
            "maxInputTokens": 128000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "inclusionAI/Ling-1T",
            "name": "Ling-1T (硅基流动)",
            "tooltip": "Ling-1T 是\"灵 2.0\"系列的首款旗舰级 non-thinking 模型，拥有 1 万亿总参数和每 token 约 500 亿个活动参数。基于灵 2.0 架构构建，Ling-1T 旨在突破高效推理和可扩展认知的极限。Ling-1T-base 在超过 20 万亿个高质量、推理密集的 token 上进行了预训练，支持高达 128K 的上下文长度，并在中段训练和后训练中采用了进化思维链（Evo-CoT）过程。这一训练课程极大地提升了模型的效率和推理深度，使 Ling-1T 能够在多个复杂推理基准上实现顶尖性能，平衡了准确性与效率",
            "maxInputTokens": 128000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "ByteDance-Seed/Seed-OSS-36B-Instruct",
            "name": "Seed-OSS-36B-Instruct (硅基流动)",
            "tooltip": "Seed-OSS 是由字节跳动 Seed 团队开发的一系列开源大型语言模型，专为强大的长上下文处理、推理、智能体（agent）和通用能力而设计。该系列中的 Seed-OSS-36B-Instruct 是一个拥有 360 亿参数的指令微调模型，它原生支持超长上下文长度，使其能够一次性处理海量文档或复杂的代码库。该模型在推理、代码生成和智能体任务（如工具使用）方面进行了特别优化，同时保持了平衡且出色的通用能力。此模型的一大特色是\"思考预算\"（Thinking Budget）功能，允许用户根据需要灵活调整推理长度，从而在实际应用中有效提升推理效率。",
            "maxInputTokens": 256000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        }
    ]
}
