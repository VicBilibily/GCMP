{
    "displayName": "无问芯穹 (EOL)",
    "baseUrl": "https://cloud.infini-ai.com/maas/v1",
    "apiKeyTemplate": "sk-xxxxxxxxxxxxxxxx",
    "models": [
        {
            "id": "deepseek-v3.2-exp",
            "name": "DeepSeek-V3.2-Exp (无问芯穹)",
            "tooltip": "DeepSeek-V3.2-Exp 是 DeepSeek 系列的实验版本模型，作为迈向下一代架构的中间步骤，V3.2-Exp 在 V3.1-Terminus 的基础上引入了 DeepSeek 稀疏注意力机制——一种旨在探索和验证在长上下文场景中训练和推理效率优化的稀疏注意力机制。",
            "maxInputTokens": 128000,
            "maxOutputTokens": 64000,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "deepseek-v3.1-terminus",
            "name": "DeepSeek-V3.1-Terminus (无问芯穹)",
            "tooltip": "DeepSeek-V3.1-Terminus 在保持模型原有能力的同时，解决了用户报告的一些问题，包括：语言一致性：减少中英文混杂文本和偶尔出现的异常字符；代理能力：进一步优化代码代理和搜索代理的性能。",
            "maxInputTokens": 128000,
            "maxOutputTokens": 64000,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "deepseek-v3.1",
            "name": "DeepSeek-V3.1 (无问芯穹)",
            "tooltip": "DeepSeek-V3.1 是一个混合模型，支持思考模式和非思考模式。总参数量为 671B，每个 Token 激活 37B 参数。该模型在 DeepSeek-V3.1-Base 的基础上进行后训练，通过两阶段长上下文扩展方法构建，显著扩展了数据集规模。",
            "maxInputTokens": 128000,
            "maxOutputTokens": 32000,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "deepseek-v3",
            "name": "DeepSeek-V3-0324 (无问芯穹)",
            "tooltip": "DeepSeek-V3-0324 是一个强大的专家混合（MoE）语言模型，总参数量约为 660B（Huggingface 上为 685 B），每个 Token 激活 37B 参数。该模型采用多头潜在注意力（MLA）和 DeepSeekMoE 架构，实现了高效推理和经济训练，并在前代 DeepSeek-V3 的基础上显著提升了性能。",
            "maxInputTokens": 128000,
            "maxOutputTokens": 16000,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "glm-4.6",
            "name": "GLM-4.6 (无问芯穹)",
            "tooltip": "GLM-4.6 是智谱 AI 最新发布的旗舰模型，相比 GLM-4.5 带来了多项关键改进。该模型在推理、编程和智能体任务方面表现出色，为开发者提供了更强大的 AI 能力。",
            "maxInputTokens": 200000,
            "maxOutputTokens": 64000,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "glm-4.5",
            "name": "GLM-4.5 (无问芯穹)",
            "tooltip": "GLM-4.5 是智谱 AI 推出的专为智能体设计的基础模型。GLM-4.5 拥有 3550 亿总参数量，其中 320 亿活跃参数。该模型统一了推理、编码和智能体能力，以满足智能体应用的复杂需求。",
            "maxInputTokens": 128000,
            "maxOutputTokens": 96000,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "glm-4.5-air",
            "name": "GLM-4.5-Air (无问芯穹)",
            "tooltip": "GLM-4.5-Air 是智谱 AI 推出的 GLM-4.5 系列中的轻量化版本。GLM-4.5-Air 采用更紧凑的设计，拥有 1060 亿总参数量，其中 120 亿活跃参数。该模型在保持优异效率的同时，仍能提供强大的推理、编码和智能体能力。",
            "maxInputTokens": 128000,
            "maxOutputTokens": 96000,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "glm-4.5v",
            "name": "GLM-4.5V (无问芯穹)",
            "tooltip": "GLM-4.5V 是基于智谱新一代旗舰文本基座模型 GLM-4.5-Air（106B 参数，12B 激活）的视觉语言大模型。延续 GLM-4.1V-Thinking 技术路线，在 42 个公开视觉多模态榜单中综合效果达到同级别开源模型 SOTA 性能，涵盖图像、视频、文档理解以及 GUI Agent 等常见任务。",
            "maxInputTokens": 65536,
            "maxOutputTokens": 16384,
            "capabilities": {
                "toolCalling": true,
                "imageInput": true
            }
        },
        {
            "id": "kimi-k2-instruct",
            "name": "Kimi-K2-Instruct-0905 (无问芯穹)",
            "tooltip": "Kimi K2 是月之暗面推出的超大参数 MoE 架构基础模型，总参数高达 1T，激活参数为 32B。该模型在预训练阶段提出了全新的 Scaling Law 范式，通过模型稀疏度扩展定律、Muon 优化器与创新的权重裁剪方法，以及全新的合成数据策略，实现了极高的 Token 利用效率。Kimi K2-Instruct-0905 是 Kimi K2 的最新、最强大版本。",
            "maxInputTokens": 256000,
            "maxOutputTokens": 32000,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "qwen3-next-80b-a3b-instruct",
            "name": "Qwen3-Next-80B-A3B-Instruct (无问芯穹)",
            "tooltip": "Qwen3-Next-80B-A3B-Instruct 是 Qwen3-Next 系列的首发大型语言模型，总参数约 80B（激活约 3B），非嵌入参数约 79B 亿采用混合注意力（Gated DeltaNet + Gated Attention）与高稀疏度 Mixture-of-Experts（MoE）架构，面向指令遵循与通用对话场景，兼顾参数效率与推理吞吐。相比同代模型，其在知识、推理、编码、多语言与代理等多项基准上表现强劲。",
            "maxInputTokens": 128000,
            "maxOutputTokens": 32000,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "qwen3-next-80b-a3b-thinking",
            "name": "Qwen3-Next-80B-A3B-Thinking (无问芯穹)",
            "tooltip": "Qwen3-Next-80B-A3B-Thinking 是 Qwen3-Next 系列的首发思考（Thinking）大语言模型，总参数约 80B（激活约 3B），非嵌入参数约 79B，采用混合注意力（Gated DeltaNet + Gated Attention）与高稀疏度 Mixture-of-Experts（MoE）架构，面向复杂推理与超长上下文场景，兼顾参数效率与推理吞吐。该模型在复杂推理任务上表现出色，不仅优于 Qwen3-30B-A3B-Thinking-2507 与 Qwen3-32B-Thinking，还在多项基准上超越闭源模型 Gemini-2.5-Flash-Thinking，适合高复杂度与长文本处理场景。",
            "maxInputTokens": 128000,
            "maxOutputTokens": 32000,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "ernie-4.5-21b-a3b",
            "name": "ERNIE-4.5-21B-A3B (无问芯穹)",
            "tooltip": "ERNIE-4.5-21B-A3B 是百度 ERNIE 4.5 系列的文本混合专家（MoE）后训练模型，拥有 210 亿参数，每个 token 激活 30 亿参数。该模型采用异构混合专家架构，在通用语言理解、生成、数学推理和代码生成等方面表现出色。",
            "maxInputTokens": 120000,
            "maxOutputTokens": 8196,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "ernie-4.5-300b-a47b",
            "name": "ERNIE-4.5-300B-A47B (无问芯穹)",
            "tooltip": "ERNIE-4.5-300B-A47B 是百度 ERNIE 4.5 系列的旗舰级文本混合专家（MoE）后训练模型，拥有 3000 亿参数，每个 token 激活 470 亿参数。该模型代表了知识增强预训练的最高水平，在各类基准测试中达到业界领先性能，特别适合企业级大规模应用场景。",
            "maxInputTokens": 32000,
            "maxOutputTokens": 8196,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "megrez-3b-instruct",
            "name": "Megrez-3B-Instruct (无问芯穹)",
            "tooltip": "Megrez-3B-Instruct 是由无问芯穹完全自主训练的大语言模型。Megrez-3B-Instruct 旨在通过软硬协同理念，打造一款极速推理、小巧精悍、极易上手的端侧智能解决方案。",
            "maxInputTokens": 32000,
            "maxOutputTokens": 4096,
            "capabilities": {
                "toolCalling": false,
                "imageInput": false
            }
        },
        {
            "id": "qwen3-14b",
            "name": "Qwen3-14B (无问芯穹)",
            "tooltip": "Qwen3-14B 是 Qwen 系列第三代的大型语言模型，拥有 148 亿参数，专为高效推理和多语言任务设计。支持无缝切换思维模式（复杂推理）和非思维模式（通用对话），在数学、编码、常识推理及多语言指令执行中表现出色。",
            "maxInputTokens": 128000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "qwen3-235b-a22b",
            "name": "Qwen3-235B-A22B (无问芯穹)",
            "tooltip": "Qwen3-235B-A22B 是 Qwen 系列第三代的大型语言模型，采用混合专家（MoE）架构，总计 2350 亿参数，每 token 激活 220 亿参数。支持无缝切换思考模式（复杂推理）和非思考模式（通用对话），在数学、编码、常识推理及多语言指令执行中表现出色。",
            "maxInputTokens": 128000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": false,
                "imageInput": false
            }
        },
        {
            "id": "qwen3-235b-a22b-instruct-2507",
            "name": "Qwen3-235B-A22B-Instruct-2507 (无问芯穹)",
            "tooltip": "Qwen3-235B-A22B-Instruct-2507 是 Qwen3-235B-A22B 非思考模式的更新版本，采用混合专家（MoE）架构，总计 2350 亿参数，每 token 激活 220 亿参数。该模型在指令遵循、逻辑推理、文本理解、数学、科学、编程和工具使用方面显著改进。",
            "maxInputTokens": 128000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "qwen3-30b-a3b",
            "name": "Qwen3-30B-A3B (无问芯穹)",
            "tooltip": "Qwen3-30B-A3B 是 Qwen 系列第三代的大型语言模型，采用混合专家（MoE）架构，总计 305 亿参数，每 token 激活 33 亿参数。支持无缝切换思维模式（复杂推理）和非思维模式（通用对话），在数学、编码、常识推理及多语言指令执行中表现出色。",
            "maxInputTokens": 128000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "qwen3-32b",
            "name": "Qwen3-32B (无问芯穹)",
            "tooltip": "Qwen3-32B 是 Qwen 系列第三代的大型语言模型，拥有 328 亿参数，专为高效推理和多语言任务设计。支持无缝切换思考模式（复杂推理）和非思考模式（通用对话），在数学、编码、常识推理及多语言指令执行中表现出色。",
            "maxInputTokens": 128000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "qwen3-8b",
            "name": "Qwen3-8B (无问芯穹)",
            "tooltip": "Qwen3-8B 是 Qwen 系列第三代的大型语言模型，拥有 82 亿参数，专为高效推理和多语言任务设计。支持无缝切换思考模式（复杂推理）和非思考模式（通用对话），在数学、编码、常识推理及多语言指令执行中表现出色。",
            "maxInputTokens": 128000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "qwen3-coder-480b-a35b-instruct",
            "name": "Qwen3-Coder-480B-A35B-Instruct (无问芯穹)",
            "tooltip": "Qwen3-Coder-480B-A35B-Instruct 是 Qwen 系列最新的代码生成大模型，采用混合专家（MoE）架构，总计 4800 亿参数，每 token 激活 350 亿参数。该模型在代理编程、浏览器使用和基础编程任务方面表现卓越，支持 256K 长上下文理解。",
            "maxInputTokens": 128000,
            "maxOutputTokens": 32000,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "qwen3-vl-235b-a22b-instruct",
            "name": "Qwen3-VL-235B-A22B-Instruct (无问芯穹)",
            "tooltip": "Qwen3-VL-235B-A22B-Instruct 是 Qwen3-VL 系列中最强大的视觉语言模型，总参数约 235B（激活约 22B），采用 Dense 和 MoE 混合架构。该模型在文本理解与生成、视觉感知与推理、上下文长度、空间和视频动态理解以及智能体交互能力等方面实现了全面升级，支持从边缘到云端的灵活部署。",
            "maxInputTokens": 128000,
            "maxOutputTokens": 32000,
            "capabilities": {
                "toolCalling": true,
                "imageInput": true
            }
        },
        {
            "id": "qwen3-vl-235b-a22b-thinking",
            "name": "Qwen3-VL-235B-A22B-Thinking (无问芯穹)",
            "tooltip": "Qwen3-VL-235B-A22B-Thinking 是 Qwen3-VL 系列中最强大的思考增强视觉语言模型，总参数约 235B（激活约 22B），采用 Dense 和 MoE 混合架构。该模型结合了 Qwen3-VL 的强大视觉理解能力与思考推理机制，在文本理解与生成、视觉感知与推理、上下文长度、空间和视频动态理解以及智能体交互能力等方面实现了全面升级。",
            "maxInputTokens": 128000,
            "maxOutputTokens": 32000,
            "capabilities": {
                "toolCalling": true,
                "imageInput": true
            }
        },
        {
            "id": "qwq-32b",
            "name": "QwQ-32B (无问芯穹)",
            "tooltip": "QwQ 是 Qwen 系列的推理模型，相比传统指令调优模型，QwQ 具备思考和推理能力，在下游任务尤其是难题上能取得显著性能提升。QwQ-32B 是一款中等规模的推理模型，其性能可与最先进的推理模型相媲美，例如 DeepSeek-R1 和 o1-mini。",
            "maxInputTokens": 64000,
            "maxOutputTokens": 8192,
            "capabilities": {
                "toolCalling": true,
                "imageInput": false
            }
        },
        {
            "id": "step3",
            "name": "Step3 (无问芯穹)",
            "tooltip": "Step3 是尖端的多模态推理模型，基于专家混合架构构建，拥有 3210 亿总参数量，其中 380 亿活跃参数。该模型从端到端设计，旨在最小化解码成本，同时在视觉-语言推理任务中提供顶级性能。",
            "maxInputTokens": 64000,
            "maxOutputTokens": 64000,
            "capabilities": {
                "toolCalling": true,
                "imageInput": true
            }
        }
    ]
}
