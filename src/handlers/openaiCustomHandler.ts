/*---------------------------------------------------------------------------------------------
 *  OpenAI è‡ªå®šä¹‰ SSE å¤„ç†å™¨
 *  ä½¿ç”¨åŸç”Ÿ fetch API å’Œè‡ªå®šä¹‰ SSE æµå¤„ç†ï¼Œæ”¯æŒ reasoning_content ç­‰æ‰©å±•å­—æ®µ
 *--------------------------------------------------------------------------------------------*/

import * as vscode from 'vscode';
import OpenAI from 'openai';
import { Logger, isIFlowGatewayURL, applyIFlowGatewayHeaders } from '../utils';
import { ConfigManager } from '../utils/configManager';
import { ApiKeyManager } from '../utils/apiKeyManager';
import { TokenUsagesManager } from '../usages/usagesManager';
import { ModelConfig, ProviderConfig } from '../types/sharedTypes';
import { StreamReporter } from './streamReporter';

/**
 * OpenAI Handler æ¥å£ï¼ˆç”¨äºç±»å‹å®‰å…¨çš„æ¶ˆæ¯å’Œå·¥å…·è½¬æ¢ï¼‰
 */
interface IOpenAIHandler {
    convertMessagesToOpenAI(
        messages: readonly vscode.LanguageModelChatMessage[],
        modelConfig?: ModelConfig
    ): OpenAI.Chat.ChatCompletionMessageParam[];
    convertToolsToOpenAI(tools: vscode.LanguageModelChatTool[]): OpenAI.Chat.ChatCompletionTool[];
}

/**
 * æ‰©å±•Deltaç±»å‹ä»¥æ”¯æŒreasoning_contentå­—æ®µ
 */
export interface ExtendedDelta extends OpenAI.Chat.ChatCompletionChunk.Choice.Delta {
    reasoning_content?: string;
}

/**
 * æ‰©å±•çš„ CompletionUsage æ¥å£ï¼ŒåŒ…å« prompt_tokens_details å’Œ completion_tokens_details
 */
interface ExtendedCompletionUsage extends OpenAI.Completions.CompletionUsage {
    prompt_tokens_details?: {
        cached_tokens?: number;
        audio_tokens?: number;
        [key: string]: number | undefined;
    };
    completion_tokens_details?: {
        reasoning_tokens?: number;
        audio_tokens?: number;
        [key: string]: number | undefined;
    };
}

/**
 * OpenAI è‡ªå®šä¹‰ SSE å¤„ç†å™¨
 * ä½¿ç”¨åŸç”Ÿ fetch API å’Œè‡ªå®šä¹‰ SSE æµå¤„ç†
 */
export class OpenAICustomHandler {
    constructor(
        private provider: string,
        private providerConfig: ProviderConfig,
        private openaiHandler: IOpenAIHandler
    ) {}

    /**
     * ä½¿ç”¨è‡ªå®šä¹‰ SSE æµå¤„ç†çš„è¯·æ±‚æ–¹æ³•
     */
    async handleRequest(
        model: vscode.LanguageModelChatInformation,
        modelConfig: ModelConfig,
        messages: readonly vscode.LanguageModelChatMessage[],
        options: vscode.ProvideLanguageModelChatResponseOptions,
        progress: vscode.Progress<vscode.LanguageModelResponsePart2>,
        token: vscode.CancellationToken,
        requestId?: string | null
    ): Promise<void> {
        const provider = modelConfig.provider || this.provider;
        const apiKey = await ApiKeyManager.getApiKey(provider);
        if (!apiKey) {
            throw new Error(`ç¼ºå°‘ ${provider} API å¯†é’¥`);
        }

        const baseURL = modelConfig.baseUrl || 'https://api.openai.com/v1';
        const url = `${baseURL}/chat/completions`;

        Logger.info(`[${model.name}] å¤„ç† ${messages.length} æ¡æ¶ˆæ¯ï¼Œä½¿ç”¨è‡ªå®šä¹‰ SSE å¤„ç†`);

        if (!this.openaiHandler) {
            throw new Error('OpenAI å¤„ç†å™¨æœªåˆå§‹åŒ–');
        }

        // æ„å»ºè¯·æ±‚å‚æ•°
        const requestBody: OpenAI.Chat.ChatCompletionCreateParamsStreaming = {
            model: modelConfig.model || model.id,
            messages: this.openaiHandler.convertMessagesToOpenAI(messages, modelConfig),
            max_tokens: ConfigManager.getMaxTokensForModel(model.maxOutputTokens),
            stream: true,
            stream_options: { include_usage: true },
            temperature: ConfigManager.getTemperature(),
            top_p: ConfigManager.getTopP()
        };

        // æ·»åŠ å·¥å…·æ”¯æŒï¼ˆå¦‚æœæœ‰ï¼‰
        if (options.tools && options.tools.length > 0 && modelConfig.capabilities?.toolCalling) {
            requestBody.tools = this.openaiHandler.convertToolsToOpenAI([...options.tools]);
        }

        // åˆå¹¶ extraBody å‚æ•°ï¼ˆå¦‚æœæœ‰ï¼‰
        if (modelConfig.extraBody) {
            const filteredExtraBody = modelConfig.extraBody;
            Object.assign(requestBody, filteredExtraBody);
            Logger.trace(`${model.name} åˆå¹¶äº† extraBody å‚æ•°: ${JSON.stringify(filteredExtraBody)}`);
        }

        Logger.debug(`[${model.name}] å‘é€ API è¯·æ±‚`);

        const abortController = new AbortController();
        const cancellationListener = token.onCancellationRequested(() => abortController.abort());

        try {
            // åˆå¹¶æä¾›å•†çº§åˆ«å’Œæ¨¡å‹çº§åˆ«çš„ customHeader
            // æ¨¡å‹çº§åˆ«çš„ customHeader ä¼šè¦†ç›–æä¾›å•†çº§åˆ«çš„åŒåå¤´éƒ¨
            const mergedCustomHeader = {
                ...this.providerConfig?.customHeader,
                ...modelConfig?.customHeader
            };

            // å¤„ç†åˆå¹¶åçš„ customHeader ä¸­çš„ API å¯†é’¥æ›¿æ¢
            const processedCustomHeader = ApiKeyManager.processCustomHeader(mergedCustomHeader, apiKey);

            const headers: Record<string, string> = {
                'Content-Type': 'application/json',
                Authorization: `Bearer ${apiKey}`,
                ...processedCustomHeader
            };

            // æ³¨å…¥ iFlow ç½‘å…³ç­¾åå¤´
            if (baseURL && isIFlowGatewayURL(baseURL)) {
                await applyIFlowGatewayHeaders(headers, apiKey, provider);
            }

            const response = await fetch(url, {
                method: 'POST',
                headers: headers,
                body: JSON.stringify(requestBody),
                signal: abortController.signal
            });

            if (!response.ok) {
                const errorText = await response.text();
                let errorMessage = `APIè¯·æ±‚å¤±è´¥: ${response.status} ${response.statusText}`;

                // å°è¯•è§£æé”™è¯¯å“åº”ï¼Œæå–è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                try {
                    const errorJson = JSON.parse(errorText);
                    if (errorJson.error) {
                        if (typeof errorJson.error === 'string') {
                            errorMessage = errorJson.error;
                        } else if (errorJson.error.message) {
                            errorMessage = errorJson.error.message;
                        }
                    }
                } catch {
                    // å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹é”™è¯¯æ–‡æœ¬
                    if (errorText) {
                        errorMessage = `${errorMessage} - ${errorText}`;
                    }
                }

                throw new Error(errorMessage);
            }

            if (!response.body) {
                throw new Error('å“åº”ä½“ä¸ºç©º');
            }

            // åˆ›å»ºç»Ÿä¸€çš„æµæŠ¥å‘Šå™¨
            const reporter = new StreamReporter({
                modelName: model.name,
                modelId: model.id,
                provider: this.provider,
                sdkMode: 'openai',
                progress
            });

            await this.processStream(model, response.body, reporter, requestId || '', token);

            Logger.debug(`[${model.name}] APIè¯·æ±‚å®Œæˆ`);
        } catch (error) {
            if (error instanceof Error && error.name === 'AbortError') {
                Logger.warn(`[${model.name}] ç”¨æˆ·å–æ¶ˆäº†è¯·æ±‚`);
                throw new vscode.CancellationError();
            }
            throw error;
        } finally {
            cancellationListener.dispose();
        }
    }

    /**
     * å¤„ç† SSE æµ
     */
    private async processStream(
        model: vscode.LanguageModelChatInformation,
        body: ReadableStream<Uint8Array>,
        reporter: StreamReporter,
        requestId: string,
        token: vscode.CancellationToken
    ): Promise<void> {
        const reader = body.getReader();
        const decoder = new TextDecoder();
        let buffer = '';
        let chunkCount = 0;

        // Token ç»Ÿè®¡: æ”¶é›† usage ä¿¡æ¯
        let finalUsage: ExtendedCompletionUsage | undefined;
        // è®°å½•æµå¤„ç†çš„å¼€å§‹å’Œç»“æŸæ—¶é—´
        let streamStartTime: number | undefined = undefined;

        try {
            while (true) {
                if (token.isCancellationRequested) {
                    Logger.warn(`[${model.name}] ç”¨æˆ·å–æ¶ˆäº†è¯·æ±‚`);
                    break;
                }

                const { done, value } = await reader.read();
                if (done) {
                    break;
                }

                // è®°å½•é¦–ä¸ª chunk çš„æ—¶é—´ä½œä¸ºæµå¼€å§‹æ—¶é—´
                if (streamStartTime === undefined) {
                    streamStartTime = Date.now();
                }

                buffer += decoder.decode(value, { stream: true });
                const lines = buffer.split('\n');
                buffer = lines.pop() || '';

                for (const line of lines) {
                    if (!line.trim() || line.trim() === '') {
                        continue;
                    }

                    // å¤„ç† SSE æ•°æ®è¡Œ
                    if (line.startsWith('data:')) {
                        const data = line.substring(5).trim();

                        if (data === '[DONE]') {
                            Logger.debug(`[${model.name}] æ”¶åˆ°æµç»“æŸæ ‡è®°`);
                            continue;
                        }

                        try {
                            const chunk = JSON.parse(data);
                            chunkCount++;

                            // æå–å“åº” IDï¼ˆä»é¦–ä¸ª chunkï¼‰
                            if (chunk.id && typeof chunk.id === 'string') {
                                reporter.setResponseId(chunk.id);
                            }

                            // æ£€æŸ¥æ˜¯å¦æ˜¯åŒ…å« usage ä¿¡æ¯çš„æœ€ç»ˆ chunk
                            if (chunk.usage) {
                                finalUsage = chunk.usage;
                            }

                            // å¤„ç†æ­£å¸¸çš„ choices
                            for (const choice of chunk.choices || []) {
                                const delta = choice.delta as ExtendedDelta | undefined;

                                // å¤„ç†æ€è€ƒå†…å®¹ï¼ˆreasoning_contentï¼‰
                                if (delta && delta.reasoning_content && typeof delta.reasoning_content === 'string') {
                                    reporter.bufferThinking(delta.reasoning_content);
                                }

                                // å¤„ç†æ–‡æœ¬å†…å®¹
                                if (delta && delta.content && typeof delta.content === 'string') {
                                    reporter.reportText(delta.content);
                                }

                                // å¤„ç†å·¥å…·è°ƒç”¨ - æ”¯æŒåˆ†å—æ•°æ®çš„ç´¯ç§¯å¤„ç†
                                if (delta && delta.tool_calls && Array.isArray(delta.tool_calls)) {
                                    for (const toolCall of delta.tool_calls) {
                                        const toolIndex = toolCall.index ?? 0;
                                        reporter.accumulateToolCall(
                                            toolIndex,
                                            toolCall.id,
                                            toolCall.function?.name,
                                            toolCall.function?.arguments
                                        );
                                    }
                                }

                                // æ³¨æ„ï¼šä¸åœ¨è¿™é‡Œè°ƒç”¨ flushAllï¼Œç»Ÿä¸€åœ¨æµç»“æŸæ—¶å¤„ç†
                            }
                        } catch (error) {
                            Logger.error(`[${model.name}] è§£æ JSON å¤±è´¥: ${data}`, error);
                        }
                    }
                }
            }
        } finally {
            reader.releaseLock();
        }

        // è®°å½•æµç»“æŸæ—¶é—´
        const streamEndTime = Date.now();

        // æµç»“æŸï¼Œè¾“å‡ºæ‰€æœ‰å‰©ä½™å†…å®¹
        reporter.flushAll(null);

        Logger.trace(`[${model.name}] SSE æµå¤„ç†ç»Ÿè®¡: ${chunkCount} ä¸ª chunk, hasContent=${reporter.hasContent}`);
        Logger.debug(`[${model.name}] æµå¤„ç†å®Œæˆ`);

        if (finalUsage) {
            // æå–ç¼“å­˜ token ä¿¡æ¯
            const cacheReadTokens = finalUsage.prompt_tokens_details?.cached_tokens ?? 0;
            // è®¡ç®—è¾“å‡ºé€Ÿåº¦
            const duration = streamStartTime && streamEndTime ? streamEndTime - streamStartTime : 0;
            const speed = duration > 0 ? ((finalUsage.completion_tokens / duration) * 1000).toFixed(1) : 'N/A';
            Logger.info(
                `ğŸ“Š ${model.name} Tokenä½¿ç”¨: è¾“å…¥${finalUsage.prompt_tokens}${cacheReadTokens > 0 ? ` (ç¼“å­˜:${cacheReadTokens})` : ''} + è¾“å‡º${finalUsage.completion_tokens} = æ€»è®¡${finalUsage.total_tokens}, è€—æ—¶=${duration}ms, é€Ÿåº¦=${speed} tokens/s`
            );
        }

        // === Token ç»Ÿè®¡: æ›´æ–°å®é™… token ===
        try {
            const usagesManager = TokenUsagesManager.instance;
            await usagesManager.updateActualTokens({
                requestId,
                rawUsage: finalUsage || {},
                status: 'completed',
                streamStartTime,
                streamEndTime
            });
        } catch (err) {
            Logger.warn('æ›´æ–°Tokenç»Ÿè®¡å¤±è´¥:', err);
        }
    }
}
